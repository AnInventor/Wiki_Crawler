<h1> Wiki_Crawler</h1>
Wiki Crawler will crawl through wikipedia pages and return edge list. Vertex are the web pages and edges are the links between the webpages.

Wiki_crawl.py will collect the wikipedia interlinks and returns the edge list which later can be build as graph. The output file will be in .csv format.

Text mining is done using Beautifulsoup4.

<h4>INSTALL BeautifulSoup4 first:
  
>pip install beautifulsoup4

<h4>Steps to run the wiki_crawl.py file:

>run the file.

>Enter the URL with http or https.

>Enter the number of level need to be captured.

>after completing execution the .csv file will be produced.


